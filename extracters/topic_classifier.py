import json
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
import jieba
from collections import defaultdict
import re

class EightDimensionClassifier:
    def __init__(self):
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        self.questions = {}
        
        # 8ä¸ªæ ¸å¿ƒç»´åº¦
        self.core_dimensions = {
            "ä¸ªäººä¿®å…»": {
                "keywords": ["åˆå¿ƒ", "ä¿¡å¿µ", "ç†æƒ³", "å“å¾·", "ä¿®å…»", "å“è´¨", "é“å¾·", "æƒ…æ“", 
                           "å¿ƒæ€", "æ„å¿—", "åšæŒ", "è€å¿ƒ", "ç›®æ ‡", "è¿½æ±‚", "ä»·å€¼", "äººæ ¼"],
                "sub_themes": ["é“å¾·å“è´¨", "ç†æƒ³ä¿¡å¿µ", "å¿ƒç†ç´ è´¨", "èƒ½åŠ›ä¿®å…»"]
            },
            "æ–‡å­¦è‰ºæœ¯": {
                "keywords": ["æ–‡åŒ–", "è‰ºæœ¯", "æ–‡å­¦", "ä½œå“", "åˆ›ä½œ", "å®¡ç¾", "æ„å¢ƒ", "é£æ ¼",
                           "ä¼ ç»Ÿ", "ç°ä»£", "è¡¨è¾¾", "å½¢å¼", "å†…æ¶µ", "ç²¾ç¥", "ä»·å€¼", "ä¼ æ‰¿"],
                "sub_themes": ["ä¼ ç»Ÿæ–‡åŒ–", "æ–‡å­¦åˆ›ä½œ", "è‰ºæœ¯å®¡ç¾", "æ–‡åŒ–ä¼ æ‰¿"]
            },
            "ç¤¾ä¼šæ°‘ç”Ÿ": {
                "keywords": ["ç¤¾ä¼š", "æ°‘ç”Ÿ", "äººæ°‘", "ç¾¤ä¼—", "å…¬å…±", "æœåŠ¡", "ä¿éšœ", "å…¬å¹³",
                           "æ­£ä¹‰", "å’Œè°", "ç¨³å®š", "å®‰å…¨", "æ²»ç†", "åŸºå±‚", "ç¤¾åŒº", "æƒåˆ©"],
                "sub_themes": ["æ°‘ç”Ÿä¿éšœ", "ç¤¾ä¼šæ²»ç†", "ç¤¾ä¼šå…¬å¹³", "å…¬å…±æœåŠ¡"]
            },
            "ç§‘å­¦æŠ€æœ¯": {
                "keywords": ["ç§‘æŠ€", "ç§‘å­¦", "æŠ€æœ¯", "åˆ›æ–°", "ç ”å‘", "æ™ºèƒ½", "æ•°å­—", "ä¿¡æ¯",
                           "è¿›æ­¥", "å‘å±•", "ç ”ç©¶", "æ¢ç´¢", "å‘æ˜", "åˆ›é€ ", "åº”ç”¨", "çªç ´"],
                "sub_themes": ["ç§‘æŠ€åˆ›æ–°", "æŠ€æœ¯åº”ç”¨", "ç§‘å­¦ç ”ç©¶", "æ•°å­—æ™ºèƒ½"]
            },
            "ç”Ÿæ€ç¯ä¿": {
                "keywords": ["ç”Ÿæ€", "ç¯å¢ƒ", "ä¿æŠ¤", "è‡ªç„¶", "èµ„æº", "å¯æŒç»­", "ç»¿è‰²", "æ¸…æ´",
                           "æ±¡æŸ“", "æ²»ç†", "æ°”å€™", "èƒ½æº", "å¾ªç¯", "ä½ç¢³", "ç¾ä¸½", "å®¶å›­"],
                "sub_themes": ["ç¯å¢ƒä¿æŠ¤", "å¯æŒç»­å‘å±•", "ç”Ÿæ€å»ºè®¾", "èµ„æºåˆ©ç”¨"]
            },
            "äº§ä¸šå‘å±•": {
                "keywords": ["äº§ä¸š", "å‘å±•", "ç»“æ„", "å‡çº§", "è½¬å‹", "é“¾æ¡", "é›†ç¾¤", "ç°ä»£åŒ–",
                           "åˆ¶é€ ä¸š", "æœåŠ¡ä¸š", "å†œä¸š", "å·¥ä¸š", "ä¼ä¸š", "å¸‚åœº", "ç«äº‰", "åˆ›æ–°"],
                "sub_themes": ["äº§ä¸šå‡çº§", "ç»“æ„è°ƒæ•´", "åˆ›æ–°å‘å±•", "ç°ä»£åŒ–äº§ä¸š"]
            },
            "æ”¿æ²»æ–‡åŒ–": {
                "keywords": ["æ”¿æ²»", "æ–‡åŒ–", "æ–‡æ˜", "ç²¾ç¥", "ä»·å€¼", "æ€æƒ³", "ç†è®º", "åˆ¶åº¦",
                           "æ²»ç†", "é¢†å¯¼", "æ°‘ä¸»", "æ³•æ²»", "å›½å®¶", "æ°‘æ—", "è®¤åŒ", "è‡ªä¿¡"],
                "sub_themes": ["æ”¿æ²»ç†è®º", "æ–‡åŒ–å»ºè®¾", "ä»·å€¼è§‚å¿µ", "å›½å®¶æ²»ç†"]
            },
            "ç»æµæ”¿ç­–": {
                "keywords": ["ç»æµ", "æ”¿ç­–", "å‘å±•", "å¢é•¿", "å¸‚åœº", "è°ƒæ§", "æ”¹é©", "å¼€æ”¾",
                           "ä½“åˆ¶", "æœºåˆ¶", "é‡‘è", "æŠ•èµ„", "æ¶ˆè´¹", "ä¾›ç»™", "ç¨³å®š", "å¹³è¡¡"],
                "sub_themes": ["å®è§‚ç»æµ", "æ”¿ç­–è°ƒæ§", "æ”¹é©å¼€æ”¾", "å¸‚åœºæœºåˆ¶"]
            }
        }
    
    def load_data(self, file_path):
        """åŠ è½½æ•°æ®"""
        with open(file_path, 'r', encoding='utf-8') as f:
            self.questions = json.load(f)
        print(f"æˆåŠŸåŠ è½½ {len(self.questions)} ä¸ªé—®é¢˜")
    
    def extract_dimension_keywords(self, text):
        """æå–ç»´åº¦å…³é”®è¯"""
        words = jieba.cut(text)
        found_keywords = []
        
        for word in words:
            for dimension, info in self.core_dimensions.items():
                if word in info['keywords'] and len(word) > 1:
                    found_keywords.append((word, dimension))
        
        return found_keywords
    
    def calculate_dimension_scores(self, text):
        """è®¡ç®—å„ç»´åº¦å¾—åˆ†"""
        scores = {dimension: 0 for dimension in self.core_dimensions}
        
        # å…³é”®è¯åŒ¹é…å¾—åˆ†
        keywords = self.extract_dimension_keywords(text)
        for word, dimension in keywords:
            scores[dimension] += 1
        
        # æ–‡æœ¬å†…å®¹åŒ¹é…å¾—åˆ†
        for dimension, info in self.core_dimensions.items():
            for keyword in info['keywords']:
                if keyword in text:
                    scores[dimension] += 0.5
        
        return scores
    
    def predict_dimension(self, text, options_text=""):
        """é¢„æµ‹é—®é¢˜ç»´åº¦"""
        combined_text = text + " " + options_text
        scores = self.calculate_dimension_scores(combined_text)
        
        # æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„ç»´åº¦
        if scores:
            max_score = max(scores.values())
            if max_score > 0:
                best_dimensions = [dim for dim, score in scores.items() if score == max_score]
                best_dimension = best_dimensions[0]  # å–ç¬¬ä¸€ä¸ª
                
                # ç¡®å®šå­ä¸»é¢˜
                sub_theme = self.predict_sub_theme(best_dimension, combined_text)
                
                return {
                    'main_dimension': best_dimension,
                    'sub_theme': sub_theme,
                    'confidence': min(max_score / 5, 1.0),  # æ ‡å‡†åŒ–ç½®ä¿¡åº¦
                    'dimension_scores': scores
                }
        
        return {
            'main_dimension': 'å…¶ä»–',
            'sub_theme': 'æœªåˆ†ç±»',
            'confidence': 0.0,
            'dimension_scores': scores
        }
    
    def predict_sub_theme(self, dimension, text):
        """é¢„æµ‹å­ä¸»é¢˜"""
        sub_themes = self.core_dimensions[dimension]['sub_themes']
        theme_keywords_map = {
            "ä¸ªäººä¿®å…»": {
                "é“å¾·å“è´¨": ["å“å¾·", "é“å¾·", "å“è´¨", "ä¿®å…»", "æƒ…æ“"],
                "ç†æƒ³ä¿¡å¿µ": ["ç†æƒ³", "ä¿¡å¿µ", "ä¿¡ä»°", "è¿½æ±‚", "ç›®æ ‡"],
                "å¿ƒç†ç´ è´¨": ["å¿ƒæ€", "å¿ƒç†", "æƒ…ç»ª", "æ„å¿—", "åšæŒ"],
                "èƒ½åŠ›ä¿®å…»": ["èƒ½åŠ›", "ç´ è´¨", "æœ¬é¢†", "æ‰å¹²", "æ™ºæ…§"]
            },
            "æ–‡å­¦è‰ºæœ¯": {
                "ä¼ ç»Ÿæ–‡åŒ–": ["ä¼ ç»Ÿ", "æ–‡åŒ–", "å†å²", "é—äº§", "ä¼ æ‰¿"],
                "æ–‡å­¦åˆ›ä½œ": ["æ–‡å­¦", "åˆ›ä½œ", "ä½œå“", "è¡¨è¾¾", "é£æ ¼"],
                "è‰ºæœ¯å®¡ç¾": ["è‰ºæœ¯", "å®¡ç¾", "æ„å¢ƒ", "å½¢å¼", "å†…æ¶µ"],
                "æ–‡åŒ–ä¼ æ‰¿": ["ä¼ æ‰¿", "å‘å±•", "åˆ›æ–°", "ç°ä»£", "ä»·å€¼"]
            },
            "ç¤¾ä¼šæ°‘ç”Ÿ": {
                "æ°‘ç”Ÿä¿éšœ": ["æ°‘ç”Ÿ", "ä¿éšœ", "å°±ä¸š", "æ•™è‚²", "åŒ»ç–—"],
                "ç¤¾ä¼šæ²»ç†": ["æ²»ç†", "ç®¡ç†", "æœåŠ¡", "å…¬å…±", "åŸºå±‚"],
                "ç¤¾ä¼šå…¬å¹³": ["å…¬å¹³", "æ­£ä¹‰", "å¹³ç­‰", "æƒåˆ©", "æœºä¼š"],
                "å…¬å…±æœåŠ¡": ["æœåŠ¡", "å…¬å…±", "ç¾¤ä¼—", "äººæ°‘", "éœ€æ±‚"]
            },
            "ç§‘å­¦æŠ€æœ¯": {
                "ç§‘æŠ€åˆ›æ–°": ["åˆ›æ–°", "åˆ›é€ ", "ç ”å‘", "çªç ´", "å‘æ˜"],
                "æŠ€æœ¯åº”ç”¨": ["æŠ€æœ¯", "åº”ç”¨", "ä½¿ç”¨", "å®è·µ", "æ•ˆæœ"],
                "ç§‘å­¦ç ”ç©¶": ["ç§‘å­¦", "ç ”ç©¶", "æ¢ç´¢", "å‘ç°", "ç†è®º"],
                "æ•°å­—æ™ºèƒ½": ["æ•°å­—", "æ™ºèƒ½", "ä¿¡æ¯", "ç½‘ç»œ", "æ•°æ®"]
            },
            "ç”Ÿæ€ç¯ä¿": {
                "ç¯å¢ƒä¿æŠ¤": ["ç¯å¢ƒ", "ä¿æŠ¤", "æ±¡æŸ“", "æ²»ç†", "æ¸…æ´"],
                "å¯æŒç»­å‘å±•": ["å¯æŒç»­", "å‘å±•", "èµ„æº", "èŠ‚çº¦", "å¾ªç¯"],
                "ç”Ÿæ€å»ºè®¾": ["ç”Ÿæ€", "å»ºè®¾", "è‡ªç„¶", "ç³»ç»Ÿ", "å¹³è¡¡"],
                "èµ„æºåˆ©ç”¨": ["èµ„æº", "åˆ©ç”¨", "èƒ½æº", "å¼€å‘", "èŠ‚çº¦"]
            },
            "äº§ä¸šå‘å±•": {
                "äº§ä¸šå‡çº§": ["å‡çº§", "è½¬å‹", "ä¼˜åŒ–", "æå‡", "ç°ä»£åŒ–"],
                "ç»“æ„è°ƒæ•´": ["ç»“æ„", "è°ƒæ•´", "å¸ƒå±€", "é…ç½®", "åè°ƒ"],
                "åˆ›æ–°å‘å±•": ["åˆ›æ–°", "å‘å±•", "åˆ›é€ ", "çªç ´", "è¿›æ­¥"],
                "ç°ä»£åŒ–äº§ä¸š": ["ç°ä»£", "äº§ä¸š", "ä½“ç³»", "é“¾æ¡", "é›†ç¾¤"]
            },
            "æ”¿æ²»æ–‡åŒ–": {
                "æ”¿æ²»ç†è®º": ["æ”¿æ²»", "ç†è®º", "æ€æƒ³", "ä¸»ä¹‰", "åŸç†"],
                "æ–‡åŒ–å»ºè®¾": ["æ–‡åŒ–", "å»ºè®¾", "ç²¾ç¥", "ä»·å€¼", "æ–‡æ˜"],
                "ä»·å€¼è§‚å¿µ": ["ä»·å€¼", "è§‚å¿µ", "ç†å¿µ", "ä¿¡å¿µ", "è¿½æ±‚"],
                "å›½å®¶æ²»ç†": ["å›½å®¶", "æ²»ç†", "åˆ¶åº¦", "æ³•æ²»", "æ°‘ä¸»"]
            },
            "ç»æµæ”¿ç­–": {
                "å®è§‚ç»æµ": ["ç»æµ", "å®è§‚", "å‘å±•", "å¢é•¿", "ç¨³å®š"],
                "æ”¿ç­–è°ƒæ§": ["æ”¿ç­–", "è°ƒæ§", "å¹²é¢„", "å¼•å¯¼", "æ”¯æŒ"],
                "æ”¹é©å¼€æ”¾": ["æ”¹é©", "å¼€æ”¾", "ä½“åˆ¶", "æœºåˆ¶", "åˆ›æ–°"],
                "å¸‚åœºæœºåˆ¶": ["å¸‚åœº", "æœºåˆ¶", "ç«äº‰", "ä¾›æ±‚", "ä»·æ ¼"]
            }
        }
        
        scores = {theme: 0 for theme in sub_themes}
        for theme in sub_themes:
            for keyword in theme_keywords_map[dimension][theme]:
                if keyword in text:
                    scores[theme] += 1
        
        if scores:
            max_score = max(scores.values())
            if max_score > 0:
                best_themes = [theme for theme, score in scores.items() if score == max_score]
                return best_themes[0]
        
        return sub_themes[0]  # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ªå­ä¸»é¢˜
    
    def semantic_dimension_classification(self):
        """è¯­ä¹‰ç»´åº¦åˆ†ç±»"""
        embeddings, question_ids = self.get_text_embeddings()
        
        # ä½¿ç”¨èšç±»è¾…åŠ©åˆ†ç±»
        n_clusters = min(30, len(question_ids) // 15)
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(embeddings)
        
        classification_results = {}
        
        for i, qid in enumerate(question_ids):
            data = self.questions[qid]
            text = data['text']
            options_text = ' '.join(data['options'])
            
            # åŸºäºå…³é”®è¯å’Œè¯­ä¹‰çš„ç»´åº¦é¢„æµ‹
            dimension_prediction = self.predict_dimension(text, options_text)
            
            classification_results[qid] = {
                'main_dimension': dimension_prediction['main_dimension'],
                'sub_theme': dimension_prediction['sub_theme'],
                'confidence': dimension_prediction['confidence'],
                'cluster_id': int(clusters[i]),
                'dimension_scores': dimension_prediction['dimension_scores'],
                'keywords': list(set([kw for kw, dim in self.extract_dimension_keywords(text)]))
            }
        
        return classification_results
    
    def get_text_embeddings(self):
        """è·å–æ–‡æœ¬åµŒå…¥"""
        texts = []
        question_ids = []
        
        for qid, data in self.questions.items():
            text = data['text']
            options_text = ' '.join(data['options'])
            combined_text = f"{text} {options_text}"
            combined_text = self.preprocess_text(combined_text)
            
            texts.append(combined_text)
            question_ids.append(qid)
        
        print("æ­£åœ¨ç”Ÿæˆæ–‡æœ¬åµŒå…¥...")
        embeddings = self.model.encode(texts, show_progress_bar=True)
        return embeddings, question_ids
    
    def preprocess_text(self, text):
        """æ–‡æœ¬é¢„å¤„ç†"""
        text = re.sub(r'[^\u4e00-\u9fa5ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""ï¼ˆï¼‰ã€Šã€‹\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def save_dimension_classification(self, output_file):
        """ä¿å­˜ç»´åº¦åˆ†ç±»ç»“æœ"""
        results = self.semantic_dimension_classification()
        
        # æŒ‰ç»´åº¦ç»„ç»‡ç»“æœ
        dimension_results = defaultdict(lambda: defaultdict(dict))
        
        for qid, info in results.items():
            main_dimension = info['main_dimension']
            sub_theme = info['sub_theme']
            
            dimension_results[main_dimension][sub_theme][qid] = {
                'text': self.questions[qid]['text'],
                'options': self.questions[qid]['options'],
                'analysis': self.questions[qid]['analysis'],
                'classification_info': info
            }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dimension_results, f, ensure_ascii=False, indent=2)
        
        self.print_dimension_statistics(dimension_results)
        
        return dimension_results
    
    def print_dimension_statistics(self, dimension_results):
        """æ‰“å°ç»´åº¦ç»Ÿè®¡"""
        print("\nğŸ¯ 8ç»´åº¦åˆ†ç±»ç»Ÿè®¡ç»“æœ:")
        print("=" * 60)
        
        total_questions = sum(
            len(sub_questions) 
            for dimension in dimension_results.values() 
            for sub_questions in dimension.values()
        )
        
        print(f"æ€»é—®é¢˜æ•°: {total_questions}")
        print()
        
        for dimension, sub_themes in sorted(dimension_results.items()):
            dimension_count = sum(len(questions) for questions in sub_themes.values())
            dimension_percentage = (dimension_count / total_questions) * 100
            
            print(f"ğŸ“Š {dimension}: {dimension_count} é¢˜ ({dimension_percentage:.1f}%)")
            print("-" * 40)
            
            for sub_theme, questions in sorted(sub_themes.items(), key=lambda x: len(x[1]), reverse=True):
                sub_count = len(questions)
                sub_percentage = (sub_count / total_questions) * 100
                
                # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
                confidences = [q['classification_info']['confidence'] for q in questions.values()]
                avg_confidence = np.mean(confidences) if confidences else 0
                
                print(f"  ğŸ“ {sub_theme}: {sub_count} é¢˜ ({sub_percentage:.1f}%)")
                print(f"     å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
                
                # æ˜¾ç¤ºä»£è¡¨æ€§é¢˜ç›®
                sample_qids = list(questions.keys())[:2]
                for sample_qid in sample_qids:
                    sample_text = questions[sample_qid]['text'][:60] + "..."
                    confidence = questions[sample_qid]['classification_info']['confidence']
                    print(f"     â€¢ é—®é¢˜{sample_qid}[{confidence:.3f}]: {sample_text}")
            
            print()
    
    def analyze_dimension_distribution(self):
        """åˆ†æç»´åº¦åˆ†å¸ƒ"""
        results = self.semantic_dimension_classification()
        
        dimension_counts = defaultdict(int)
        confidence_by_dimension = defaultdict(list)
        
        for qid, info in results.items():
            dimension = info['main_dimension']
            dimension_counts[dimension] += 1
            confidence_by_dimension[dimension].append(info['confidence'])
        
        print("\nğŸ“ˆ ç»´åº¦åˆ†å¸ƒåˆ†æ:")
        print("=" * 50)
        
        total = sum(dimension_counts.values())
        for dimension, count in sorted(dimension_counts.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total) * 100
            avg_confidence = np.mean(confidence_by_dimension[dimension]) if confidence_by_dimension[dimension] else 0
            print(f"{dimension}: {count} é¢˜ ({percentage:.1f}%) - å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")

# ä½¿ç”¨ç¤ºä¾‹
def main():
    classifier = EightDimensionClassifier()
    classifier.load_data('merged_questions.json')
    
    print("å¼€å§‹8ç»´åº¦åˆ†ç±»...")
    results = classifier.save_dimension_classification('8dimension_classified.json')
    
    print("\nğŸ“Š ç»´åº¦åˆ†å¸ƒåˆ†æ...")
    classifier.analyze_dimension_distribution()
    
    print("âœ… 8ç»´åº¦åˆ†ç±»å®Œæˆï¼")

if __name__ == "__main__":
    main()
